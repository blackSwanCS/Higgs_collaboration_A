{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750332ba",
   "metadata": {},
   "source": [
    "Modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf468b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from HiggsML.ingestion import Ingestion\n",
    "from HiggsML.datasets import download_dataset\n",
    "from sample_code_submission.neural_network import NeuralNetwork\n",
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284184d",
   "metadata": {},
   "source": [
    "Charger les données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22752b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:26:34,447 - HiggsML.datasets     - INFO     - Handling as dataset name: blackSwan_data\n",
      "2025-06-04 15:26:34,456 - HiggsML.datasets     - INFO     - Current working directory: c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n",
      "2025-06-04 15:26:34,461 - HiggsML.datasets     - INFO     - Total rows: 2000000\n",
      "2025-06-04 15:26:34,461 - HiggsML.datasets     - INFO     - Test size: 600000\n",
      "2025-06-04 15:26:34,539 - HiggsML.datasets     - INFO     - Selected train size: 1400000\n",
      "2025-06-04 15:26:35,637 - HiggsML.datasets     - INFO     - Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data = download_dataset(\n",
    "    \"blackSwan_data\"\n",
    ")  # change to \"blackSwan_data\" for the actual data\n",
    "\n",
    "# load train set\n",
    "data.load_train_set()\n",
    "data_set = data.get_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596b9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_set[\"labels\"]\n",
    "weights = data_set[\"weights\"]\n",
    "detailed_label = data_set[\"detailed_labels\"]\n",
    "keys = np.unique(detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ca9ab",
   "metadata": {},
   "source": [
    "Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837cf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "submission_dir = os.path.join(root_dir, \"sample_code_submission\")\n",
    "\n",
    "# The directory where results and other outputs from the participant's code will be written\n",
    "output_dir = os.path.join(root_dir, \"sample_result_submission\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee9cee",
   "metadata": {},
   "source": [
    "Import Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c8ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(submission_dir)\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de4445",
   "metadata": {},
   "source": [
    "Testing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4461446",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SETTINGS = {\n",
    "    \"systematics\": {  # Systematics to use\n",
    "        \"tes\": False,  # tau energy scale\n",
    "        \"jes\": False,  # jet energy scale\n",
    "        \"soft_met\": False,  # soft term in MET\n",
    "        \"ttbar_scale\": False,  # W boson scale factor\n",
    "        \"diboson_scale\": False,  # Diboson scale factor\n",
    "        \"bkg_scale\": False,  # Background scale factor\n",
    "    },\n",
    "    \"num_pseudo_experiments\": 25,  # Number of pseudo-experiments to run per set\n",
    "    \"num_of_sets\": 25,  # Number of sets of pseudo-experiments to run\n",
    "}\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_settings = TEST_SETTINGS.copy()\n",
    "\n",
    "random_state = np.random.RandomState(RANDOM_SEED)\n",
    "test_settings[\"ground_truth_mus\"] = (\n",
    "    random_state.uniform(0.1, 3, test_settings[\"num_of_sets\"])\n",
    ").tolist()\n",
    "\n",
    "random_settings_file = os.path.join(output_dir, \"test_settings.json\")\n",
    "with open(random_settings_file, \"w\") as f:\n",
    "    json.dump(test_settings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5408baf",
   "metadata": {},
   "source": [
    "Boucle sur \"Ingestion\" en faisant varier epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11e4b4",
   "metadata": {},
   "source": [
    "Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1d26336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphique(S, E):\n",
    "    plt.plot(S, E, marker='o', linestyle='-', color='b')\n",
    "    plt.title('Significance en fonction de epochs')\n",
    "    plt.xlabel('significance')\n",
    "    plt.ylabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbd1f0",
   "metadata": {},
   "source": [
    "Calcul AUC : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196ee94",
   "metadata": {},
   "source": [
    "Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimization(train_data): \n",
    "    L_epochs = np.linspace(50, 50, 1)\n",
    "    sigmax = 0\n",
    "    epochs_max = 1\n",
    "    S = []\n",
    "    E = []\n",
    "    for k in range(len(L_epochs)):\n",
    "        ingestion = Ingestion(data)\n",
    "        # initialize submission\n",
    "        ingestion.init_submission(Model, \"NN\")\n",
    "        ingestion.model.model.epochs = int(L_epochs[k])\n",
    "        print(ingestion.model.model.epochs)\n",
    "        ingestion.fit_submission()\n",
    "        sig1 = ingestion.model.model.significance_2(test_labels=ingestion.model.training_set[\"labels\"],test_weights=ingestion.model.training_set[\"weights\"])\n",
    "        #visualisation des données :\n",
    "        S.append(sig1)\n",
    "        E.append(int(L_epochs[k]))\n",
    "        if sig1>sigmax: \n",
    "            sigmax = sig1\n",
    "            epochs_max = int(L_epochs[k])\n",
    "    graphique(S, E)\n",
    "    return sigmax, epochs_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d0e05",
   "metadata": {},
   "source": [
    "Il y a plusieurs problématique : \n",
    "Comment modifier une variable epochs de notre "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4458948a",
   "metadata": {},
   "source": [
    "test_labels=ingestion.model.training_set[\"labels\"],test_weights=ingestion.model.training_set[\"weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1589e",
   "metadata": {},
   "source": [
    "Lancement de la boucle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1748e",
   "metadata": {},
   "source": [
    "A l'aide des autres programmes d'optimisations, on a obtenu batchsize : 120, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0091df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b636925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:26:37,123 - HiggsML.ingestion    - INFO     - Initializing Submmited Model\n",
      "2025-06-04 15:26:37,125 - HiggsML.datasets     - INFO     - Selected train size: 50000\n",
      "2025-06-04 15:26:37,596 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 15:26:37,640 - HiggsML.datasets     - INFO     - Selected train size: 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (50000, 28)\n",
      "Training Labels:  (50000,)\n",
      "Training Weights:  (50000,)\n",
      "sum_signal_weights:  668.7998319568295\n",
      "sum_bkg_weights:  105050.2001680432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:26:38,058 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 15:26:38,077 - HiggsML.datasets     - INFO     - Selected train size: 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Data:  (50000, 28)\n",
      "Valid Labels:  (50000,)\n",
      "Valid Weights:  (50000,)\n",
      "sum_signal_weights:  666.0054770592847\n",
      "sum_bkg_weights:  105052.99452294069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:26:38,467 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 15:26:38,605 - HiggsML.ingestion    - INFO     - Calling fit method of submitted model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Data:  (50000, 28)\n",
      "Holdout Labels:  (50000,)\n",
      "Holdout Weights:  (50000,)\n",
      "sum_signal_weights:  685.479681267179\n",
      "sum_bkg_weights:  105033.52031873283\n",
      " \n",
      " \n",
      "Training Data:  (50000, 28)\n",
      "DEBUG: model_type = 'NN'\n",
      " Model is NN\n",
      "50\n",
      "Epoch 1/50\n",
      "417/417 - 2s - 6ms/step - accuracy: 0.7790 - loss: 2.0792\n",
      "Epoch 2/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8047 - loss: 1.9193\n",
      "Epoch 3/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8112 - loss: 1.8849\n",
      "Epoch 4/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8147 - loss: 1.8682\n",
      "Epoch 5/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8160 - loss: 1.8517\n",
      "Epoch 6/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8188 - loss: 1.8401\n",
      "Epoch 7/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8196 - loss: 1.8291\n",
      "Epoch 8/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8210 - loss: 1.8241\n",
      "Epoch 9/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8205 - loss: 1.8187\n",
      "Epoch 10/50\n",
      "417/417 - 1s - 2ms/step - accuracy: 0.8225 - loss: 1.8089\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36moptimization\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(L_epochs[k])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mingestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m sig1 \u001b[38;5;241m=\u001b[39m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msignificance_2(test_labels\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],test_weights\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#visualisation des données :\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\HiggsML\\ingestion.py:141\u001b[0m, in \u001b[0;36mIngestion.fit_submission\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03mFit the submitted model.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling fit method of submitted model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\\sample_code_submission\\model.py:192\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# test dataset : increase test weight to compensate for sampling\u001b[39;00m\n\u001b[0;32m    190\u001b[0m balanced_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_train\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystematics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_info \u001b[38;5;241m=\u001b[39m calculate_saved_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set)\n",
      "File \u001b[1;32mc:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\\sample_code_submission\\neural_network.py:36\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, train_data, y_train, weights_train)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit_transform(train_data)\n\u001b[0;32m     35\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(train_data)\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:375\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    373\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 375\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:740\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:111\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps_per_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, steps_per_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution):\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimization(data_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
