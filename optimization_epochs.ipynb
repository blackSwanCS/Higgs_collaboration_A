{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750332ba",
   "metadata": {},
   "source": [
    "Modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf468b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from HiggsML.ingestion import Ingestion\n",
    "from HiggsML.datasets import download_dataset\n",
    "from sample_code_submission.neural_network import NeuralNetwork\n",
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284184d",
   "metadata": {},
   "source": [
    "Charger les données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22752b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:21:33,531 - HiggsML.datasets     - INFO     - Handling as dataset name: blackSwan_data\n",
      "2025-06-04 10:21:33,544 - HiggsML.datasets     - INFO     - Current working directory: c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n",
      "2025-06-04 10:21:33,551 - HiggsML.datasets     - INFO     - Total rows: 2000000\n",
      "2025-06-04 10:21:33,551 - HiggsML.datasets     - INFO     - Test size: 600000\n",
      "2025-06-04 10:21:33,636 - HiggsML.datasets     - INFO     - Selected train size: 1400000\n",
      "2025-06-04 10:21:34,873 - HiggsML.datasets     - INFO     - Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data = download_dataset(\n",
    "    \"blackSwan_data\"\n",
    ")  # change to \"blackSwan_data\" for the actual data\n",
    "\n",
    "# load train set\n",
    "data.load_train_set()\n",
    "data_set = data.get_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596b9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_set[\"labels\"]\n",
    "weights = data_set[\"weights\"]\n",
    "detailed_label = data_set[\"detailed_labels\"]\n",
    "keys = np.unique(detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ca9ab",
   "metadata": {},
   "source": [
    "Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837cf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "submission_dir = os.path.join(root_dir, \"sample_code_submission\")\n",
    "\n",
    "# The directory where results and other outputs from the participant's code will be written\n",
    "output_dir = os.path.join(root_dir, \"sample_result_submission\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee9cee",
   "metadata": {},
   "source": [
    "Import Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c8ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(submission_dir)\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de4445",
   "metadata": {},
   "source": [
    "Testing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4461446",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SETTINGS = {\n",
    "    \"systematics\": {  # Systematics to use\n",
    "        \"tes\": False,  # tau energy scale\n",
    "        \"jes\": False,  # jet energy scale\n",
    "        \"soft_met\": False,  # soft term in MET\n",
    "        \"ttbar_scale\": False,  # W boson scale factor\n",
    "        \"diboson_scale\": False,  # Diboson scale factor\n",
    "        \"bkg_scale\": False,  # Background scale factor\n",
    "    },\n",
    "    \"num_pseudo_experiments\": 25,  # Number of pseudo-experiments to run per set\n",
    "    \"num_of_sets\": 25,  # Number of sets of pseudo-experiments to run\n",
    "}\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_settings = TEST_SETTINGS.copy()\n",
    "\n",
    "random_state = np.random.RandomState(RANDOM_SEED)\n",
    "test_settings[\"ground_truth_mus\"] = (\n",
    "    random_state.uniform(0.1, 3, test_settings[\"num_of_sets\"])\n",
    ").tolist()\n",
    "\n",
    "random_settings_file = os.path.join(output_dir, \"test_settings.json\")\n",
    "with open(random_settings_file, \"w\") as f:\n",
    "    json.dump(test_settings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5408baf",
   "metadata": {},
   "source": [
    "Boucle sur \"Ingestion\" en faisant varier epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6a2aa",
   "metadata": {},
   "source": [
    "Fonction bouclée: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196ee94",
   "metadata": {},
   "source": [
    "Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd4be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimization(train_data): \n",
    "    L_epochs = np.linspace(2, 3, 2)\n",
    "    sigmax = 0\n",
    "    epochs_max = 1\n",
    "    for k in range(len(L_epochs)):\n",
    "        path.append(submission_dir)\n",
    "        ingestion = Ingestion(data)\n",
    "        # initialize submission\n",
    "        ingestion.init_submission(Model, \"NN\")\n",
    "        ingestion.model.model.epochs = L_epochs[k]\n",
    "        print(ingestion.model.model.epochs)\n",
    "        ingestion.fit_submission()\n",
    "        sig1 = ingestion.model.model.significance_2(test_labels=ingestion.model.training_set[\"labels\"],test_weights=ingestion.model.training_set[\"weights\"])\n",
    "        #visualisation des données :\n",
    "        if sig1>sigmax: \n",
    "            sigmax = sig1\n",
    "            epochs_max = L_epochs[k]\n",
    "    return sigmax, epochs_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d0e05",
   "metadata": {},
   "source": [
    "Il y a plusieurs problématique : \n",
    "Comment modifier une variable epochs de notre "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1589e",
   "metadata": {},
   "source": [
    "Lancement de la boucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b636925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:21:36,573 - HiggsML.ingestion    - INFO     - Initializing Submmited Model\n",
      "2025-06-04 10:21:36,581 - HiggsML.datasets     - INFO     - Selected train size: 5000\n",
      "2025-06-04 10:21:36,966 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 10:21:36,986 - HiggsML.datasets     - INFO     - Selected train size: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (5000, 28)\n",
      "Training Labels:  (5000,)\n",
      "Training Weights:  (5000,)\n",
      "sum_signal_weights:  659.7971916765008\n",
      "sum_bkg_weights:  105059.20280832352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:21:37,330 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 10:21:37,346 - HiggsML.datasets     - INFO     - Selected train size: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Data:  (5000, 28)\n",
      "Valid Labels:  (5000,)\n",
      "Valid Weights:  (5000,)\n",
      "sum_signal_weights:  679.2854636254424\n",
      "sum_bkg_weights:  105039.71453637458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 10:21:37,687 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 10:21:37,874 - HiggsML.ingestion    - INFO     - Calling fit method of submitted model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Data:  (5000, 28)\n",
      "Holdout Labels:  (5000,)\n",
      "Holdout Weights:  (5000,)\n",
      "sum_signal_weights:  662.3621331136593\n",
      "sum_bkg_weights:  105056.6378668863\n",
      " \n",
      " \n",
      "Training Data:  (5000, 28)\n",
      "DEBUG: model_type = 'NN'\n",
      " Model is NN\n",
      "2.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36moptimization\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m     10\u001b[0m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m L_epochs[k]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mingestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m sig1 \u001b[38;5;241m=\u001b[39m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msignificance_2(test_labels\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],test_weights\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#visualisation des données :\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\HiggsML\\ingestion.py:141\u001b[0m, in \u001b[0;36mIngestion.fit_submission\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03mFit the submitted model.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling fit method of submitted model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\\sample_code_submission\\model.py:192\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# test dataset : increase test weight to compensate for sampling\u001b[39;00m\n\u001b[0;32m    190\u001b[0m balanced_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weights_train\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalanced_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystematics(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_info \u001b[38;5;241m=\u001b[39m calculate_saved_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mholdout_set)\n",
      "File \u001b[1;32mc:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\\sample_code_submission\\neural_network.py:35\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, train_data, y_train, weights_train)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mfit_transform(train_data)\n\u001b[0;32m     34\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(train_data)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\marwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m logs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    370\u001b[0m initial_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch \u001b[38;5;129;01mor\u001b[39;00m initial_epoch\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m    373\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_epoch_begin(epoch)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "optimization(data_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
