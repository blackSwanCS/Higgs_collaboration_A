{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750332ba",
   "metadata": {},
   "source": [
    "Modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf468b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from HiggsML.ingestion import Ingestion\n",
    "from HiggsML.datasets import download_dataset\n",
    "from sample_code_submission.neural_network import NeuralNetwork\n",
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284184d",
   "metadata": {},
   "source": [
    "Charger les données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22752b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:48:08,887 - HiggsML.datasets     - INFO     - Handling as dataset name: blackSwan_data\n",
      "2025-06-04 09:48:08,887 - HiggsML.datasets     - INFO     - Current working directory: c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n",
      "2025-06-04 09:48:08,899 - HiggsML.datasets     - INFO     - Total rows: 2000000\n",
      "2025-06-04 09:48:08,904 - HiggsML.datasets     - INFO     - Test size: 600000\n",
      "2025-06-04 09:48:08,987 - HiggsML.datasets     - INFO     - Selected train size: 1400000\n",
      "2025-06-04 09:48:10,077 - HiggsML.datasets     - INFO     - Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data = download_dataset(\n",
    "    \"blackSwan_data\"\n",
    ")  # change to \"blackSwan_data\" for the actual data\n",
    "\n",
    "# load train set\n",
    "data.load_train_set()\n",
    "data_set = data.get_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596b9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_set[\"labels\"]\n",
    "weights = data_set[\"weights\"]\n",
    "detailed_label = data_set[\"detailed_labels\"]\n",
    "keys = np.unique(detailed_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ca9ab",
   "metadata": {},
   "source": [
    "Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837cf465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory is c:\\Users\\marwa\\Desktop\\Neural Network\\Higgs_collaboration_A\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.getcwd()\n",
    "print(\"Root directory is\", root_dir)\n",
    "submission_dir = os.path.join(root_dir, \"sample_code_submission\")\n",
    "\n",
    "# The directory where results and other outputs from the participant's code will be written\n",
    "output_dir = os.path.join(root_dir, \"sample_result_submission\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee9cee",
   "metadata": {},
   "source": [
    "Import Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c8ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(submission_dir)\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de4445",
   "metadata": {},
   "source": [
    "Testing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4461446",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SETTINGS = {\n",
    "    \"systematics\": {  # Systematics to use\n",
    "        \"tes\": False,  # tau energy scale\n",
    "        \"jes\": False,  # jet energy scale\n",
    "        \"soft_met\": False,  # soft term in MET\n",
    "        \"ttbar_scale\": False,  # W boson scale factor\n",
    "        \"diboson_scale\": False,  # Diboson scale factor\n",
    "        \"bkg_scale\": False,  # Background scale factor\n",
    "    },\n",
    "    \"num_pseudo_experiments\": 25,  # Number of pseudo-experiments to run per set\n",
    "    \"num_of_sets\": 25,  # Number of sets of pseudo-experiments to run\n",
    "}\n",
    "\n",
    "RANDOM_SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_settings = TEST_SETTINGS.copy()\n",
    "\n",
    "random_state = np.random.RandomState(RANDOM_SEED)\n",
    "test_settings[\"ground_truth_mus\"] = (\n",
    "    random_state.uniform(0.1, 3, test_settings[\"num_of_sets\"])\n",
    ").tolist()\n",
    "\n",
    "random_settings_file = os.path.join(output_dir, \"test_settings.json\")\n",
    "with open(random_settings_file, \"w\") as f:\n",
    "    json.dump(test_settings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5408baf",
   "metadata": {},
   "source": [
    "Boucle sur \"Ingestion\" en faisant varier epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6a2aa",
   "metadata": {},
   "source": [
    "Fonction bouclée: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d61ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_epochs = np.linspace(2, 3, 2)\n",
    "sigmax = 5000\n",
    "epochs_max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196ee94",
   "metadata": {},
   "source": [
    "Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimization(train_data): \n",
    "    L_epochs = np.linspace(2, 3, 2)\n",
    "    sigmax = 5000\n",
    "    epochs_max = 1\n",
    "    for k in range(len(L_epochs)):\n",
    "        path.append(submission_dir)\n",
    "        ingestion = Ingestion(data)\n",
    "        # initialize submission\n",
    "        ingestion.init_submission(Model, \"NN\")\n",
    "        ingestion.model.model.epochs = L_epochs[k]\n",
    "        print(ingestion.model.model.epochs)\n",
    "        ingestion.fit_submission()\n",
    "        sig1 = ingestion.model.model.significance_2(test_labels=ingestion.model.training_set[\"labels\"],test_weights=ingestion.model.training_set[\"weights\"])\n",
    "        #visualisation des données :\n",
    "        if sig1>sigmax: \n",
    "            sigmax = sig1\n",
    "            epochs_max = L_epochs[k]\n",
    "    return sigmax, epochs_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1589e",
   "metadata": {},
   "source": [
    "Lancement de la boucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b636925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:52:58,301 - HiggsML.ingestion    - INFO     - Initializing Submmited Model\n",
      "2025-06-04 09:52:58,304 - HiggsML.datasets     - INFO     - Selected train size: 5000\n",
      "2025-06-04 09:52:58,675 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 09:52:58,709 - HiggsML.datasets     - INFO     - Selected train size: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (5000, 28)\n",
      "Training Labels:  (5000,)\n",
      "Training Weights:  (5000,)\n",
      "sum_signal_weights:  613.8731328026427\n",
      "sum_bkg_weights:  105105.12686719734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:52:59,057 - HiggsML.datasets     - INFO     - Data loaded successfully\n",
      "2025-06-04 09:52:59,077 - HiggsML.datasets     - INFO     - Selected train size: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid Data:  (5000, 28)\n",
      "Valid Labels:  (5000,)\n",
      "Valid Weights:  (5000,)\n",
      "sum_signal_weights:  701.3039205106508\n",
      "sum_bkg_weights:  105017.69607948934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:52:59,408 - HiggsML.datasets     - INFO     - Data loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout Data:  (5000, 28)\n",
      "Holdout Labels:  (5000,)\n",
      "Holdout Weights:  (5000,)\n",
      "sum_signal_weights:  688.3363944674295\n",
      "sum_bkg_weights:  105030.6636055326\n",
      " \n",
      " \n",
      "Training Data:  (5000, 28)\n",
      "DEBUG: model_type = 'NN'\n",
      " Model is NN\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Ingestion' object has no attribute 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moptimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m, in \u001b[0;36moptimization\u001b[1;34m(train_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m ingestion\u001b[38;5;241m.\u001b[39minit_submission(Model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m L_epochs[k]\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mingestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m)\n\u001b[0;32m     12\u001b[0m ingestion\u001b[38;5;241m.\u001b[39mfit_submission()\n\u001b[0;32m     13\u001b[0m sig1 \u001b[38;5;241m=\u001b[39m ingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msignificance_2(test_labels\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],test_weights\u001b[38;5;241m=\u001b[39mingestion\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_set[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ingestion' object has no attribute 'epochs'"
     ]
    }
   ],
   "source": [
    "optimization(data_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
